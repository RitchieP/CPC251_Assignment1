{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training model will have to split the dataset into training and test data. \n",
    "\n",
    "The training data will be used to estimate the weights, the test data will then be used to determine the loss function. In which it will also use the prediction function.\n",
    "\n",
    "https://stackoverflow.com/questions/66079043/split-dataset-without-using-scikit-learn-train-test-split\n",
    "\n",
    "https://medium.com/geekculture/linear-regression-from-scratch-in-python-without-scikit-learn-a06efe5dedb6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    "    \"\"\" Pass four arguments\n",
    "        Arguments:\n",
    "            X: input features\n",
    "            y: responses\n",
    "            alpha: learning rate\n",
    "            max_epoch: maximum epochs\n",
    "        Returns:\n",
    "            w: estimated weights\n",
    "            hist_loss: training loss history\n",
    "    \"\"\"\n",
    "    \n",
    "    hist_loss = []\n",
    "    w = []\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_dataset(X, y)\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        hist_loss.append(loss_fn(y_test, prediction(w, X)))\n",
    "\n",
    "    return w, hist_loss\n",
    "\n",
    "# Helper function to split dataset into 8:2 training to testing dataset ratio\n",
    "def split_dataset(X, y):\n",
    "    # i will be the number of training datas\n",
    "    i = int((1 - 0.2) * X.shape[0])\n",
    "\n",
    "    # Generates random numbers based on the numbers of data\n",
    "    o = np.random.permutation(X.shape[0])\n",
    "\n",
    "    \"\"\" The following lines will basically rearrange the whole X and y dataset based on the randomized order\n",
    "        Then, we will split it based on the number of training size\n",
    "    \"\"\"\n",
    "    X_train, X_test = np.split(np.take(X, o, axis=0), [i])\n",
    "    y_train, y_test = np.split(np.take(y, o), [i])\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction will return the predicted values of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, X):\n",
    "    \"\"\" Pass two arguments\n",
    "        Arguments:\n",
    "            w: weights\n",
    "            X: input features\n",
    "        Returns:\n",
    "            yhat: predicted values\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function will only return the loss of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    \"\"\" Pass two arguments\n",
    "        Arguments:\n",
    "            y: responses\n",
    "            yhat: predicted value\n",
    "        Returns:\n",
    "            loss: loss value\n",
    "    \"\"\"\n",
    "    return (y - yhat) ** 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[710 468 570 921 128 649 699 816 226 484  93 901 541 593 205 682 724  15\n",
      " 439 479 756  33 280 572 178 416  41 639 826 674 111 941 294 615 530 374\n",
      "  39 758 925 189 879 162 776 633 547 689 328 254 341 684 155 185 787 810\n",
      " 872 705 965  34 126 714  88 227 729 890 310 556 943  90 415 331 289 402\n",
      " 297 766 949 709 476  80 239 932 605 516 881 181 296 177 655 511 770  32\n",
      "  13 651 449 441 673 184 301 564 895 411 123 200 349 234 413 106  37 375\n",
      " 355 857 733  25 454 220 136 797 993 306 264 706 664 470 142 612 832 475\n",
      "  70 947 629  55 565 795 325 513 594 953 394 720 137 316 535 459 744 549\n",
      " 206 726 948 561 817 638 675  26 154 383 346  52 371 951 628 267 483 814\n",
      " 292 458 887 174 922 869 407 693 950 125 917 396 695 387 333  60 782 888\n",
      " 365  30 716 210   2 722 833 952 836 255 815 537 608 643 140  72 663 803\n",
      " 987 908 850 550 190 464 919 147 391 469 211 366 621 141 311 671 197  49\n",
      " 551 680 985 609 581 209 940 188 784 124 738  57 884 319 701 613 587 538\n",
      " 340 529  92 423 490 498 931 497 274 157 955 642 975 303  54 905 898 860\n",
      "  40 858 862 875 783 781 626 698 863   5 308 623 445 233  29 127  51 494\n",
      " 248 427 923 878  43  73 607 504 400 361 559 829 456 909 261 118 218 425\n",
      " 847 295 369 622 386  47 842 827  24 657 977 314 496 163 352 370 116 542\n",
      " 343 619  21 467 363 345 419 982  77 495 506 759 778 232  59 808 336  61\n",
      " 747 562 589  78 851 477 500  98 523 962 305 244 320 199 989 688 694 972\n",
      " 624 755 517 543 723 790 121 300 742 169 208 691 462 474 896  82  10 379\n",
      " 269 338 119 265   9 853 636 799 739 420 646 146   8 180 450  28 273 492\n",
      " 360 882 270 928 241 920 963 164 942 356 546 217 780 250 151 532 358  81\n",
      " 679 382 245 997  56 134 317 601 160 263 580 830 290 907 718 460 996 318\n",
      " 461 401 431 805 330 392 182 967 372 978 969 697 984 791 771 158 390  35\n",
      " 247 567 287 973 406 389 603 434 519 312  64 707 150 991 514 801 658 216\n",
      " 929 988 811  23 981 835 849 509 735 152 417 367  14 894 229 278 518 645\n",
      " 583 156 788 769 575 974  53 777 825 653 685 359 577 525 911 115  18  31\n",
      " 324 648 877 337 249  22 212 846 713  97 748 891 768 527 426 885 563 968\n",
      " 924 614 637 536  58 414 388 512 666 347 272 712 421 789 433 409 586 867\n",
      " 531 668 734 491 266 362 448 455 634 702 818 555 552 428 466 446 998 381\n",
      " 831 520 855 130 313 793  83  50 201 809 302 753 611 408 938 183 899 471\n",
      " 774  42 820 568  87 652 986 590  12 194 944 864 508 683 521  84 103  38\n",
      " 252 240 746  16 109 281 171 954 437 307  27 934 327 762 186 463 173 715\n",
      " 223 544 794 228 293 880 893 291 994 299 257 641 644  95 800 478 618 405\n",
      " 284 813 453 647 741 576 731 870 897 773  91  20 960 203 170 760 488 761\n",
      " 686 198 510 524 650 443 796  62 807 775 271 260 798 873 187 202 779 395\n",
      " 230 114 717 326 672 859 627 432 876 132 616 588 596 854 889 167 727 743\n",
      " 736 322 848 222 259 721 298 958 380 687 192 285 412 256   0 936 481  89\n",
      " 191 597 403 964 422 235  48 752 144 667 268 507 243 465 632 861 971 107\n",
      " 335 225 139 824 502 545 812 112 837 838 892 350 321 916 772  63  69 540\n",
      " 487  66 385 696  94 737   4 238  11 566 995 429 499 571 207 393 122 915\n",
      " 246  44 874 354 631 102 635 224 105 763 749 438 447 959 620 910 640   3\n",
      " 283 153  68 999 856 528 526  45 935 329 927 665 617 585 503 553   7 214\n",
      " 802 745 676 133 138  67 332 754 976 179 681 493 654 430 323 786 276 595\n",
      " 213 983 886 970 604 251 104  79 913 533 334 161 819 193 275 131 309 444\n",
      " 398 452 253 579 740 591 410 912 145 792 843 168  99  36 961 669 933 677\n",
      " 592 486   1 364 373 418 129 630 757  19 834 165 473 926 558 149 522 765\n",
      " 117 204 236 262 670 108 196 304 903  46 515 424 900 610 821 113 806 548\n",
      " 377 956 578 822 110 501 937 868 573 732 557 730  71 584 823 930 176 451\n",
      " 750  17 661 215 660 348 914 378 436 703 904 582 435 574 258 600 120 751\n",
      " 442 883 599 195   6 692 659 946 539 221 700 845 286 397 440 704 399 534\n",
      " 804  65 602 828 980  75 785 353 992 764 482 166 100 279 288 918 711 277\n",
      " 342 376 560 841 866 351 143 505 368 457 135 725 690 728 282 554 852 625\n",
      " 237 966 357 979 839 404 606 315 159 384 902 662 569  85 906 175  96 339\n",
      " 101 865 656 708 844 231 219 242 472 719 598  74 840 939 480 990 945 767\n",
      " 148 678  86  76 344 485 172 957 489 871]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read data from csv file\n",
    "    data = pd.read_csv(\"assignment1_dataset.csv\", sep=\",\")\n",
    "    data = data[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"response\"]]\n",
    "    predict = \"response\"\n",
    "\n",
    "    \n",
    "    x = data.drop([predict], 1)\n",
    "    y = data[predict]\n",
    "\n",
    "    print(np.random.permutation(x.shape[0]))\n",
    "\n",
    "    # w, hist_loss = train_model(x, y, 0.2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
