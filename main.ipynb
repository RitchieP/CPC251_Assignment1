{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training model will have to split the dataset into training and test data. \n",
    "\n",
    "The training data will be used to estimate the weights, the test data will then be used to determine the loss function. In which it will also use the prediction function.\n",
    "\n",
    "https://stackoverflow.com/questions/66079043/split-dataset-without-using-scikit-learn-train-test-split\n",
    "\n",
    "https://medium.com/geekculture/linear-regression-from-scratch-in-python-without-scikit-learn-a06efe5dedb6\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-implement-a-gradient-descent-in-python-to-find-a-local-minimum/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    "    \"\"\" Pass four arguments\n",
    "        Arguments:\n",
    "            X: input features\n",
    "            y: responses\n",
    "            alpha: learning rate\n",
    "            max_epoch: maximum epochs\n",
    "        Returns:\n",
    "            w: estimated weights\n",
    "            hist_loss: training loss history\n",
    "    \"\"\"\n",
    "    \n",
    "    hist_loss = []\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_dataset(X, y)\n",
    "    # Initialize weights with zero with the amount of columns I have\n",
    "    current_weight = np.array(np.random.randint(0, 9, x.columns.shape))\n",
    "    current_bias = 0.01\n",
    "\n",
    "    # We calculate the loss function for the first randomly initialized weights, and store it\n",
    "    current_loss = loss_fn(y_test, prediction(w, X_test))\n",
    "    hist_loss.append(current_loss)\n",
    "    n = len(X_train)\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Estimate the parameter using the randomly initialized weights and loss function for it\n",
    "        w = gradient_descent(w, alpha, current_loss, n)\n",
    "\n",
    "        # Predict with the new weights, and calculate the loss function for it, and store it\n",
    "        y_pred = prediction(w, X_test)\n",
    "        current_loss = loss_fn(y_test, y_pred)\n",
    "        hist_loss.append(loss_fn(y_test, y_pred))\n",
    "\n",
    "    return w, hist_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to split dataset into 8:2 training to testing dataset ratio\n",
    "def split_dataset(X, y):\n",
    "    # i will be the number of training datas\n",
    "    i = int((1 - 0.2) * X.shape[0])\n",
    "\n",
    "    # Generates random numbers based on the numbers of data\n",
    "    o = np.random.permutation(X.shape[0])\n",
    "\n",
    "    \"\"\" The following lines will basically rearrange the whole X and y dataset based on the randomized order\n",
    "        Then, we will split it based on the number of training size\n",
    "    \"\"\"\n",
    "    X_train, X_test = np.split(np.take(X, o, axis=0), [i])\n",
    "    y_train, y_test = np.split(np.take(y, o), [i])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Helper function to calculate estimate parameters by gradient descent\n",
    "#TODO: Solve this gradient descent function \n",
    "def gradient_descent(weights, alpha, n):\n",
    "    weights = weights - alpha * (-(2/n) * sum())\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction will return the predicted values of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, X):\n",
    "    \"\"\" Pass two arguments\n",
    "        Arguments:\n",
    "            w: weights\n",
    "            X: input features\n",
    "        Returns:\n",
    "            yhat: predicted values\n",
    "    \"\"\"\n",
    "    yhat = []\n",
    "\n",
    "    # Removing the string header\n",
    "    new_header = X.iloc[0] #grab the first row for the header\n",
    "    new_X = X[1:] #take the data less the header row\n",
    "    new_X.columns = new_header #set the header row as the df header\n",
    "\n",
    "    # TODO: This is should be the number of columns and yhat should append the number of rows\n",
    "    for i in range(len(X)):\n",
    "        yhat.append(sum(w * new_X))\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function will only return the loss of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    \"\"\" Pass two arguments\n",
    "        Arguments:\n",
    "            y: responses\n",
    "            yhat: predicted value\n",
    "        Returns:\n",
    "            loss: loss value\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    loss = (1/n) * sum(y - yhat) ** 2\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "diff requires input that is at least one dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-b6cfc1e52873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhist_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-1a2f7591d731>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X, y, alpha, max_epoch)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Estimate the parameter using the randomly initialized weights and loss function for it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Predict with the new weights, and calculate the loss function for it, and store it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-0142076f1b33>\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(weights, alpha, lossFn)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Helper function to calculate estimate parameters by gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossFn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlossFn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[0;32m   1233\u001b[0m     \u001b[0mnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"diff requires input that is at least one dimensional\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: diff requires input that is at least one dimensional"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read data from csv file\n",
    "    data = pd.read_csv(\"assignment1_dataset.csv\", sep=\",\")\n",
    "    data = data[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"response\"]]\n",
    "    predict = \"response\"\n",
    "\n",
    "    \n",
    "    x = data.drop([predict], 1)\n",
    "    y = data[predict]\n",
    "\n",
    "    weights, hist_loss = train_model(x, y, alpha=0.1, max_epoch=100)\n",
    "    print(hist_loss)\n",
    "\n",
    "    # testing purposes\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = split_dataset(x, y)\n",
    "    # # Initialize weights with zero with the amount of columns I have\n",
    "    # w = np.array(np.random.randint(0, 9, x.columns.shape))\n",
    "\n",
    "    # new_header = X_test.iloc[0] #grab the first row for the header\n",
    "    # new_X_test = X_test[1:] #take the data less the header row\n",
    "    # new_X_test.columns = new_header #set the header row as the df header\n",
    "\n",
    "    # print(sum(w * new_X_test))\n",
    "    # print(X_test)\n",
    "\n",
    "\n",
    "    # We calculate the loss function for the first randomly initialized weights, and store it\n",
    "    # current_loss = loss_fn(y_test, prediction(w, X_test))\n",
    "\n",
    "    # w, hist_loss = train_model(x, y, 0.2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
